{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "# Charger les données\n",
    "X_train = pd.read_csv('kaggle_data/train_x.csv', index_col=0)\n",
    "y_train = pd.read_csv('kaggle_data/train_y.csv')\n",
    "X_test = pd.read_csv('kaggle_data/test_x.csv')\n",
    "X_val = pd.read_csv('kaggle_data/val_x.csv')\n",
    "y_val = pd.read_csv('kaggle_data/val_y.csv')\n",
    "\n",
    "#Change type of string column to string\n",
    "X_train['string'] = X_train['string'].astype(str)\n",
    "X_test['string'] = X_test['string'].astype(str)\n",
    "X_val['string'] = X_val['string'].astype(str)\n",
    "\n",
    "\n",
    "#Sample dataset\n",
    "X_train_sample = X_train[:10]\n",
    "y_train_sample = y_train[:10]\n",
    "X_val_sample = X_val[:10]\n",
    "y_val_sample = y_val[:10]\n",
    "\n",
    "def train_bert_model(target):\n",
    "    \n",
    "    # Get the target column\n",
    "    y_target = y_train_sample[target]\n",
    "    \n",
    "    # Initialize the BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    # Tokenize the input data\n",
    "    inputs = tokenizer(X_train_sample['string'].tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Create the dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(y_target.tolist()))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Set up the optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(dataloader) * 10\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            progress_bar.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "    # Set the name of the model\n",
    "    model_name = f\"model_BERT_{target}\"\n",
    "    \n",
    "    return model, model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 1/1 [00:12<00:00, 12.44s/it, Loss=0.639]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:07<00:00,  7.56s/it, Loss=0.606]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it, Loss=0.548]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:08<00:00,  8.12s/it, Loss=0.538]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:08<00:00,  8.07s/it, Loss=0.463]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:06<00:00,  6.88s/it, Loss=0.494]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:09<00:00,  9.04s/it, Loss=0.478]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:09<00:00,  9.77s/it, Loss=0.423]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:08<00:00,  8.94s/it, Loss=0.471]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it, Loss=0.413]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_male')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_model('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:09<00:00,  9.59s/it, Loss=0.871]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:11<00:00, 11.26s/it, Loss=0.701]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:17<00:00, 17.21s/it, Loss=0.592]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:19<00:00, 19.63s/it, Loss=0.533]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:19<00:00, 19.28s/it, Loss=0.487]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:16<00:00, 16.83s/it, Loss=0.413]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:12<00:00, 12.26s/it, Loss=0.441]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, Loss=0.384]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.37s/it, Loss=0.325]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it, Loss=0.306]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:14<00:00, 14.36s/it, Loss=0.545]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:15<00:00, 15.91s/it, Loss=0.533]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:13<00:00, 13.80s/it, Loss=0.502]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:09<00:00,  9.86s/it, Loss=0.441]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:13<00:00, 13.83s/it, Loss=0.426]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:14<00:00, 14.82s/it, Loss=0.427]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:13<00:00, 13.23s/it, Loss=0.378]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:16<00:00, 16.92s/it, Loss=0.348]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:16<00:00, 16.51s/it, Loss=0.358]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:19<00:00, 19.02s/it, Loss=0.35]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:20<00:00, 20.85s/it, Loss=0.809]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:18<00:00, 18.48s/it, Loss=0.512]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:12<00:00, 12.41s/it, Loss=0.406]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:15<00:00, 15.17s/it, Loss=0.337]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:16<00:00, 16.01s/it, Loss=0.306]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:16<00:00, 16.41s/it, Loss=0.288]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:15<00:00, 15.78s/it, Loss=0.302]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:15<00:00, 15.34s/it, Loss=0.291]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:15<00:00, 15.25s/it, Loss=0.289]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:13<00:00, 13.03s/it, Loss=0.255]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:07<00:00,  7.60s/it, Loss=0.46]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:10<00:00, 10.55s/it, Loss=0.386]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it, Loss=0.365]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:20<00:00, 20.66s/it, Loss=0.332]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:23<00:00, 23.04s/it, Loss=0.273]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:22<00:00, 22.79s/it, Loss=0.246]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:11<00:00, 11.77s/it, Loss=0.244]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:13<00:00, 13.72s/it, Loss=0.242]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:29<00:00, 29.62s/it, Loss=0.229]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:19<00:00, 19.30s/it, Loss=0.207]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:13<00:00, 13.70s/it, Loss=0.913]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it, Loss=0.791]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:12<00:00, 12.12s/it, Loss=0.667]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:09<00:00,  9.66s/it, Loss=0.591]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:08<00:00,  8.69s/it, Loss=0.548]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it, Loss=0.523]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it, Loss=0.503]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, Loss=0.457]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:13<00:00, 13.34s/it, Loss=0.403]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:35<00:00, 35.55s/it, Loss=0.384]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:14<00:00, 14.22s/it, Loss=0.647]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:23<00:00, 23.41s/it, Loss=0.569]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:13<00:00, 13.25s/it, Loss=0.474]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:21<00:00, 21.48s/it, Loss=0.405]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it, Loss=0.341]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:12<00:00, 12.13s/it, Loss=0.302]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:12<00:00, 12.83s/it, Loss=0.266]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:19<00:00, 19.44s/it, Loss=0.246]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:23<00:00, 23.26s/it, Loss=0.229]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:14<00:00, 14.28s/it, Loss=0.218]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:14<00:00, 14.57s/it, Loss=0.72]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:14<00:00, 14.43s/it, Loss=0.574]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:14<00:00, 14.82s/it, Loss=0.575]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:17<00:00, 17.84s/it, Loss=0.449]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:16<00:00, 16.46s/it, Loss=0.454]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:15<00:00, 15.94s/it, Loss=0.373]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:13<00:00, 13.53s/it, Loss=0.367]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:08<00:00,  8.16s/it, Loss=0.348]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it, Loss=0.356]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it, Loss=0.368]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:07<00:00,  7.50s/it, Loss=0.858]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:07<00:00,  7.46s/it, Loss=0.677]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:07<00:00,  7.22s/it, Loss=0.622]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it, Loss=0.493]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:07<00:00,  7.72s/it, Loss=0.479]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it, Loss=0.408]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it, Loss=0.403]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:07<00:00,  7.23s/it, Loss=0.435]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:06<00:00,  6.38s/it, Loss=0.374]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:05<00:00,  5.80s/it, Loss=0.411]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_BERT_male': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_female': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_LGBTQ': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_christian': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_muslim': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_other_religions': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_black': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " 'model_BERT_white': BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Create the \"models\" directory if it doesn't exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "toxicity_categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions','black', 'white']\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for category in toxicity_categories:\n",
    "    model, model_name = train_bert_model(category)\n",
    "    trained_models[model_name]=model\n",
    "    model.save_pretrained(f\"{model_name}\")\n",
    "\n",
    "trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/theob/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_BERT_male': {'Accuracy': 0.94,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_female': {'Accuracy': 0.89,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_LGBTQ': {'Accuracy': 0.99,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_christian': {'Accuracy': 0.97,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_muslim': {'Accuracy': 0.91,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_other_religions': {'Accuracy': 0.94,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_black': {'Accuracy': 0.93,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0},\n",
       " 'model_BERT_white': {'Accuracy': 0.94,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1': 0.0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Change type of string column to string\n",
    "X_val['string'] = X_val['string'].astype(str)\n",
    "\n",
    "X_val_sample = X_val[:100]\n",
    "y_val_sample = y_val[:100]\n",
    "\n",
    "categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions','black', 'white']\n",
    "# Iterate over the trained models\n",
    "evaluation_results = {}\n",
    "for (model_name, model), category in zip(trained_models.items(), categories):\n",
    "    # Tokenize the input data\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(X_val_sample['string'].tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Create the dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(y_val_sample[category].tolist()))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(predicted_labels.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    precision = precision_score(targets, predictions)\n",
    "    recall = recall_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    \n",
    "    # Store the evaluation results\n",
    "    evaluation_results[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
    "\n",
    "evaluation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
