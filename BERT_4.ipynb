{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torchmetrics import AUROC, F1Score\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, max_length=512):\n",
    "        super(BertDataset, self).__init__()\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
    "\n",
    "        if mode != 'test':\n",
    "            self.labels = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.iloc[idx, 0])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        if hasattr(self, 'labels'):\n",
    "            label = self.labels.iloc[idx].tolist()  # Assuming labels are in separate columns per class\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BERT_trained = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=17)\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('checkpoint_1706230483.6858711.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dictionary into your model\n",
    "model_BERT_trained.load_state_dict(state_dict)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model_BERT_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead8aed6f70c4b3987def454c11f31f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = 'kaggle_data'\n",
    "test_dataset = BertDataset(data_dir, 'test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 20, shuffle = False)\n",
    "def pred_BERT_model(model, dataloader):\n",
    "    # Passer le modèle en mode évaluation\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_indices = []\n",
    "\n",
    "    # Pas besoin de calculer les gradients pour l'inférence\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, leave=False):\n",
    "            input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "\n",
    "            # Faire des prédictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Collecter les logits et convertir en probabilités si nécessaire\n",
    "            pred = torch.sigmoid(outputs.logits).detach().cpu().tolist()\n",
    "            all_predictions.extend(pred)\n",
    "            # Gérer les indices pour chaque prédiction\n",
    "            if 'index' in batch:\n",
    "                all_indices.extend(batch['index'].tolist())  # Directement convertir si 'index' est présent\n",
    "            else:\n",
    "                all_indices.extend(list(range(len(pred))))  # Utiliser range converti en liste sinon\n",
    "\n",
    "    # Créer un DataFrame pour les prédictions\n",
    "    pred_df = pd.DataFrame(all_predictions, index=all_indices)\n",
    "\n",
    "    # La fonction retourne maintenant uniquement le DataFrame des prédictions\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "\n",
    "pred_df, label_df = pred_BERT_model(model_BERT_trained, test_dataloader )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
